{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= './aclImdb_v1/aclImdb'\n",
    "folders= ['train', 'test']\n",
    "\n",
    "def read_data(path):\n",
    "    srch_path= os.path.join(path, \"*.txt\")\n",
    "    print(\"Searching for \"+ srch_path)\n",
    "    files= glob.glob(srch_path)\n",
    "    print(\"number of files: \"+ str(len(files)))\n",
    "    data= []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as fp:\n",
    "            content= fp.read()\n",
    "            data.append(content)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for ./aclImdb_v1/aclImdb/train/pos/*.txt\n",
      "number of files: 12500\n",
      "Searching for ./aclImdb_v1/aclImdb/train/neg/*.txt\n",
      "number of files: 12500\n"
     ]
    }
   ],
   "source": [
    "train_pos_reviews= read_data(os.path.join(data_dir,'train','pos'))\n",
    "train_neg_reviews= read_data(os.path.join(data_dir,'train','neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews= train_pos_reviews + train_neg_reviews\n",
    "train_labels= [1]* len(train_pos_reviews) + [0] * len(train_neg_reviews)\n",
    "\n",
    "train_reviews, train_labels= shuffle(train_reviews, train_labels, random_state= 711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer= BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'it', ':', ')']\n",
      "['Go', 'baby', 'GO', 'BA', '##B', '##Y', 'it', 'is', 'a', 'great', 'movie']\n"
     ]
    }
   ],
   "source": [
    "sentences= [\"I like it :) \", \"Go baby GO BABY it is a great movie\"]\n",
    "for sent in sentences:\n",
    "    tokens= tokenizer.tokenize(sent)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   146,  1176,  1122,   131,   114,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [  101,  3414,  2963, 27157, 12465,  2064,  3663,  1122,  1110,   170,\n",
       "          1632,  2523,   102,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences,\n",
    "         padding= 'max_length',\n",
    "         max_length= 15,\n",
    "         return_tensors= 'pt',\n",
    "         return_token_type_ids= False\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "102\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.sep_token_id)\n",
    "print(tokenizer.cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, reviews, labels, tokenizer, max_len):\n",
    "        self.reviews= reviews\n",
    "        self.labels= labels\n",
    "        self.tokenizer= tokenizer\n",
    "        self.max_len= max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review= self.reviews[item]\n",
    "        review= BeautifulSoup(review, \"html.parser\").get_text()\n",
    "        label= self.labels[item]\n",
    "        \n",
    "        encodings= tokenizer(\n",
    "                review,\n",
    "                padding= 'max_length',\n",
    "                max_length= self.max_len,\n",
    "                truncation= True,\n",
    "                return_tensors= 'pt'\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            #'review': review,\n",
    "            'encoding': encodings,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_data= ReviewDataset(sentences, [1,0], tokenizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': {'input_ids': tensor([[  101,  3414,  2963, 27157,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])},\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoader(dataset, tokenizer, max_len= 150, batch_size= 32, num_workers=0):\n",
    "    '''\n",
    "    dataset is a dictionary with 2 keys 'data', 'labels'\n",
    "    '''\n",
    "    encoded_dataset= ReviewDataset(dataset['data'], dataset['labels'], tokenizer, max_len)\n",
    "    \n",
    "    return DataLoader(encoded_dataset, batch_size= batch_size, num_workers= num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= {'data': train_reviews, 'labels':train_labels}\n",
    "train_dataloader= createDataLoader(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "dl_item= next(iter(train_dataloader))\n",
    "print(len(dl_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': {'input_ids': tensor([[[  101,  1135,   112,  ...,     0,     0,     0]],\n",
       "  \n",
       "          [[  101,   107,  1109,  ...,  1341, 11679,   102]],\n",
       "  \n",
       "          [[  101,  1109,  5855,  ...,     0,     0,     0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[  101,  9913,  5098,  ...,   188,   107,   102]],\n",
       "  \n",
       "          [[  101,  6963,  2029,  ...,  1145,  9178,   102]],\n",
       "  \n",
       "          [[  101,  1448,  1104,  ...,   112,   188,   102]]]),\n",
       "  'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]]]),\n",
       "  'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]]])},\n",
       " 'label': tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 0, 1])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 150])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_tensor= dl_item['encoding']['input_ids']\n",
    "ip_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_item['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cb6ae0cb9a48baa6dcbe4e960a18ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7495df9945747ffbb8a8ca26c9d0f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 150])\n",
      "torch.Size([32, 1, 150])\n"
     ]
    }
   ],
   "source": [
    "ip_id_tensor= dl_item['encoding']['input_ids']\n",
    "attention_mask_tensor= dl_item['encoding']['attention_mask']\n",
    "print(ip_id_tensor.shape)\n",
    "print(attention_mask_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 150])\n",
      "torch.Size([32, 150])\n"
     ]
    }
   ],
   "source": [
    "ip_id_tensor= ip_id_tensor.squeeze()\n",
    "attention_mask_tensor= attention_mask_tensor.squeeze()\n",
    "print(ip_id_tensor.shape)\n",
    "print(attention_mask_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "  input_ids= ip_id_tensor, \n",
    "  attention_mask= attention_mask_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
