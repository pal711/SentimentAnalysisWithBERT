{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: boto3 1.12.34 has requirement botocore<1.16.0,>=1.15.34, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.34 has requirement botocore==1.15.34, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "#import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir= './aclImdb_v1/aclImdb'\n",
    "data_dir= 's3://sagemaker-ap-south-1-831906679170/aclImdb/'\n",
    "folders= ['train', 'test']\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "def read_data(path):\n",
    "    #srch_path= os.path.join(path, \"*.txt\")\n",
    "    #print(\"Searching for \"+ srch_path)\n",
    "    #files= glob.glob(srch_path)\n",
    "    files= fs.ls(path)\n",
    "    print(\"number of files: \"+ str(len(files)))\n",
    "    data= []\n",
    "    for file in files:\n",
    "        with fs.open(file, 'r') as fp:\n",
    "            content= fp.read()\n",
    "            data.append(content)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sagemaker-ap-south-1-831906679170/aclImdb/train/pos/0_9.txt',\n",
       " 'sagemaker-ap-south-1-831906679170/aclImdb/train/pos/10000_8.txt',\n",
       " 'sagemaker-ap-south-1-831906679170/aclImdb/train/pos/10001_10.txt',\n",
       " 'sagemaker-ap-south-1-831906679170/aclImdb/train/pos/10002_7.txt',\n",
       " 'sagemaker-ap-south-1-831906679170/aclImdb/train/pos/10003_8.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d='s3://sagemaker-ap-south-1-831906679170/aclImdb/train/pos/'\n",
    "fs.ls(d)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files: 12500\n",
      "number of files: 12500\n"
     ]
    }
   ],
   "source": [
    "train_pos_reviews= read_data(os.path.join(data_dir,'train','pos'))\n",
    "train_neg_reviews= read_data(os.path.join(data_dir,'train','neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews= train_pos_reviews + train_neg_reviews\n",
    "train_labels= [1]* len(train_pos_reviews) + [0] * len(train_neg_reviews)\n",
    "\n",
    "train_reviews, train_labels= shuffle(train_reviews, train_labels, random_state= 711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./train_reviews.pkl\",'wb') as fp:\n",
    "    pickle.dump(train_reviews, fp)\n",
    "    \n",
    "with open(\"./train_labels.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(train_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ae36d8bac24178bf7f98bc7d0f79de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer= BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'it', ':', ')']\n",
      "['go', 'baby', 'go', 'baby', 'it', 'is', 'a', 'great', 'movie']\n"
     ]
    }
   ],
   "source": [
    "sentences= [\"I like it :) \", \"Go baby GO BABY it is a great movie\"]\n",
    "for sent in sentences:\n",
    "    tokens= tokenizer.tokenize(sent)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2066, 2009, 1024, 1007,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0],\n",
       "        [ 101, 2175, 3336, 2175, 3336, 2009, 2003, 1037, 2307, 3185,  102,    0,\n",
       "            0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences,\n",
    "         padding= 'max_length',\n",
    "         max_length= 15,\n",
    "         return_tensors= 'pt',\n",
    "         return_token_type_ids= False\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "102\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.sep_token_id)\n",
    "print(tokenizer.cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, reviews, labels, tokenizer, max_len):\n",
    "        self.reviews= reviews\n",
    "        self.labels= labels\n",
    "        self.tokenizer= tokenizer\n",
    "        self.max_len= max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review= self.reviews[item]\n",
    "        review= BeautifulSoup(review, \"html.parser\").get_text()\n",
    "        label= self.labels[item]\n",
    "        \n",
    "        encodings= tokenizer(\n",
    "                review,\n",
    "                padding= 'max_length',\n",
    "                max_length= self.max_len,\n",
    "                truncation= True,\n",
    "                return_tensors= 'pt'\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            #'review': review,\n",
    "            'encoding': encodings,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_data= ReviewDataset(sentences, [1,0], tokenizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': {'input_ids': tensor([[ 101, 2175, 3336, 2175,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])},\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoader(dataset, tokenizer, max_len= 150, batch_size= 32, num_workers=0):\n",
    "    '''\n",
    "    dataset is a dictionary with 2 keys 'data', 'labels'\n",
    "    '''\n",
    "    encoded_dataset= ReviewDataset(dataset['data'], dataset['labels'], tokenizer, max_len)\n",
    "    \n",
    "    return DataLoader(encoded_dataset, batch_size= batch_size, num_workers= num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "25000\n",
      "<class 'list'>\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/train_reviews.pkl\",'rb') as fp:\n",
    "    labeled_reviews= pickle.load(fp)\n",
    "\n",
    "with open(\"./data/train_labels.pkl\",'rb') as fp:\n",
    "    labeled_labels= pickle.load(fp)\n",
    "\n",
    "print(type(labeled_reviews))\n",
    "print(len(labeled_reviews))\n",
    "\n",
    "print(type(labeled_labels))\n",
    "print(len(labeled_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_reviews, val_reviews, train_labels, val_labels= train_test_split(\n",
    "    labeled_reviews,\n",
    "    labeled_labels,\n",
    "    random_state= 711,\n",
    "    train_size= 0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "Like last year, I didn't manage to sit through the whole thing. Okay, so Chris Rock as a host was a good choice because he was vaguely engaging. Or rather, out of all the total bores packed into the theatre, he at least wasn't in the Top 10 Most Boring. A lot of the presenters, on the other hand, were in this coveted Top 10. I hadn't known that the whole thing had been done by autocue (although I knew it was scripted) but it was really terrible to see these supposedly good actors unable to insert expression, look away from the cue and stumble over simple words (Natalie PortmanÂ…",
      "if there's no director, she's gone). The Night of Fancy Dresses and Boring Speeches was long and tedious, Beyonce Knowles butchered some good songs and there were very few decent acceptance speeches and clips. Adam Sandler wins the Worst Presenter award.<br /><br />For helping me write this review I'd like to thank my Mum, my Dad, my lawyers and my pedicurist for all believing in me, and I'd like to point out that I have a high metabolism and of course I haven't been starving myself for a month. I'm not going to cry...thank you.\n",
      "\n",
      "5000\n",
      "I have copies of both these Movies the classic where Robert blake is a mighty fine actor where most of the 1967 movie Blake is more shown standing by a window in jail telling his childhood life where it makes since why he killed the Clutter Family doesn't show much in the classic of what really went on an doesn't tell us which one really done the killing but it's a great eye catcher really if you watch the 1996 movie In cold Blood the classic makes a lot more sence .\n"
     ]
    }
   ],
   "source": [
    "print(len(train_reviews))\n",
    "print(train_reviews[0])\n",
    "print()\n",
    "print(len(val_reviews))\n",
    "print(val_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train_reviews_v2.pkl\",'wb') as fp:\n",
    "    pickle.dump(train_reviews, fp)\n",
    "    \n",
    "with open(\"./data/val_reviews_v2.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(val_reviews, fp)\n",
    "    \n",
    "with open(\"./data/train_labels_v2.pkl\",'wb') as fp:\n",
    "    pickle.dump(train_labels, fp)\n",
    "    \n",
    "with open(\"./data/val_labels_v2.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(val_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= {'data': train_reviews, 'labels':train_labels}\n",
    "train_dataloader= createDataLoader(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "dl_item= next(iter(train_dataloader))\n",
    "print(len(dl_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': {'input_ids': tensor([[[  101,  2066,  2197,  ...,  1012,  1996,   102]],\n",
       "  \n",
       "          [[  101,  2023,  2143,  ...,  3302,  1010,   102]],\n",
       "  \n",
       "          [[  101,  1037,  6703,  ...,  2057,  2963,   102]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[  101,  2074,  2128,  ...,  1011,  6052,   102]],\n",
       "  \n",
       "          [[  101,  8870, 23185,  ...,  2860,  2860,   102]],\n",
       "  \n",
       "          [[  101,  8235,  2338,  ...,     0,     0,     0]]]),\n",
       "  'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0]]]),\n",
       "  'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "  \n",
       "          [[1, 1, 1,  ..., 0, 0, 0]]])},\n",
       " 'label': tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 150])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_tensor= dl_item['encoding']['input_ids']\n",
    "ip_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_item['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9830c7e08b764adc812c47393a0f5973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f0f1969a6d46f79e463c9ed2fa94f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 150])\n",
      "torch.Size([32, 1, 150])\n"
     ]
    }
   ],
   "source": [
    "ip_id_tensor= dl_item['encoding']['input_ids']\n",
    "attention_mask_tensor= dl_item['encoding']['attention_mask']\n",
    "print(ip_id_tensor.shape)\n",
    "print(attention_mask_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 150])\n",
      "torch.Size([32, 150])\n"
     ]
    }
   ],
   "source": [
    "ip_id_tensor= ip_id_tensor.squeeze()\n",
    "attention_mask_tensor= attention_mask_tensor.squeeze()\n",
    "print(ip_id_tensor.shape)\n",
    "print(attention_mask_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_val= bert_model(\n",
    "  input_ids= ip_id_tensor, \n",
    "  attention_mask= attention_mask_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ret_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 150, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_val.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_val.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
